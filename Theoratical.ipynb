{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoratical View to Neural Networks\n",
    "I'm more application based when it comes to Data Science -- I'm pretty good with applying the mechanics of it, but I must be honest, I'm not the best with memorizing and remembering the theory.\n",
    "\n",
    "And yet, we'd miss a lot of the magic on why Neural Networks work if we ignore the theory. So let's dive into a super basic premise of what a Neural Network actually is.\n",
    "\n",
    "I would be a liar if I said I did this entire Notebook by myself. A lot of it -- especially the images -- were taken from Seth Weidman, who presented a tutorial at PyData NYC 2017. I encourage you to follow him at www.sethweidman.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Why Neural Networks?\n",
    "So why a Neural Network? Why not Machine Learning?\n",
    "\n",
    "### Visual Example\n",
    "Let's say you have the world's most simplistic dataset which looks like this, and you're trying to classify the colored points.\n",
    "![DataPlot](img/xor.png)\n",
    "\n",
    "There isn't a linear classification or logistic classification method to really classify these plots, without being totally absurd or incorrect -- we cannot draw a line, or even a region, to seperate these two classes. \n",
    "\n",
    "\n",
    "### Math Example\n",
    "Alternatively, let's say you have three points of data, each one containing up to three boolean (i.e. True/False) inputs like this:\n",
    "$$ N(1, 0, 0) = 1 $$\n",
    "$$ N(0, 1, 0) = 1 $$\n",
    "$$ N(1, 1, 0) = 1 $$\n",
    "\n",
    "The third function messes up our ability to use Logistic Regression. In other words, there is no parameter b, w1, w2, or w3 such that:\n",
    "$$N(x_1, x_2, x_3) = \\frac{1}{1 + e^{b + w_1 * x_1 + w_2 * x_2 + w_3 * x_3}}$$\n",
    "\n",
    "\n",
    "### Feature Engineering\n",
    "We could use **Feature Engineering** to solve this. To explain Feature Engineering in a few words, it's manually designing what the input should be. You might try to add some discrimination algorithms or emphasize some key features, to modify your inputs so that its key features become more obvious.\n",
    "\n",
    "Deep Learning -- which includes Neural Networks -- is honestly something like **Architecture Engineering** (don't Google that term, I made it up). In Neural Networks, we're effectively playing around either with the parameters or the architecture of our Neural Network so that it better fits our data. In effect, we end up training the computer to do Feature Engineering on its own.\n",
    "\n",
    "Which solution is better? The new hotness is Deep Learning and letting the computer do the feature engineering rather than us. In practice though, there is a time and place for feature engineering. If you suspect your dataset could be defined with a linear or logistic regression by finetuning some of the inputs, it'll probably be faster from a computation perspective to stick to feature engineering and then slide into machine learning. There's not a good 'one rule' here -- whether you go Feature Engieering + ML or go straight to DL is up to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# What is a Neural Network: Forward Propogation\n",
    "Introducing one of the most classic diagrams in visualizing what a Neural Network looks like:\n",
    "![Neural_Intro](img/neural_net_basic.png)\n",
    "**Disclaimer: I had a typo on this diagram. The second set of weights on the right should be 'w', not 'v'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Applying Weights\n",
    "The first step most Neural Networks take is to take the inputs, multiply it by some weight, to obtain a feature.\n",
    "\n",
    "$$ a_1 = x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} $$\n",
    "$$ a_2 = x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} $$\n",
    "$$ a_3 = x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} $$\n",
    "$$ a_4 = x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} $$\n",
    "\n",
    "In the first pass, these weights are probably something absurdly silly (like all 1s, all 0s, etc.). Overtime, they get refined into more helpful units, and we'll talk more about this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "The second step is passing the feature through an **Activation Function**. This is important because if we stuck with the above functions through the entire process, we'd stay Linear and create a Linear Transformation. Chances are, you're here because you want a non-linear transformation and this is where Activation Function comes in.\n",
    "\n",
    "There are many 'premade' Activation Functions and many Activation Functions that are being researched. Here is an example of an activation function that you will almost certainly use as the last function in any classification problem. This is the **Sigmoid Function** which maps numbers from 0 to 1.\n",
    "\n",
    "$$ B = \\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Here's a list of all the possible Activation Functions. Later, we'll talk about the **Relu Function** which is arguably the most popular activation function, although the reason it works so well will probably surprise you.\n",
    "\n",
    "<img src='img/Activation.tiff'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "The Loss is the difference between the estimated result and actual result. Mathematically, this is often the **Mean Squared Error Loss**. The lower the loss is, the better.\n",
    "\n",
    "$$ L = \\frac{1}{2}(y - P)^2 $$\n",
    "\n",
    "There are other Loss Functions you can use and we'll talk more about them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# What is a Neural Network: Forwards to Backwards\n",
    "We continue these algorithms noted above, repeating them until we reach the end of the diagram I showed you before.\n",
    "\n",
    "But you might ask how the weights change -- because by default, they start at silly & unhelpful states. These weights do refine in a process we call `Backpropogation` and/or `Backwards Propogation` (depending who you talk to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does Backpropogation work?\n",
    "Recall that a Neural Network is effectively, some function that takes the inputs of the last function and passes its output to the next function.\n",
    "\n",
    "\\begin{align}\n",
    "A &= a(x, V) \\\\\n",
    "B &= b(A) \\\\\n",
    "C &= c(B, W) \\\\\n",
    "P &= p(C) \\\\\n",
    "L &= l(P)\n",
    "\\end{align}\n",
    "\n",
    "Because these equations are so linked, we can combine them in one line such as below:\n",
    "\n",
    "$$ L = l(p(c(b(a(x, V)), W))) $$\n",
    "\n",
    "Notice that both Weights (W & V) are related to the functions which create the Neural Network. This implies that **the calculation of those weights in the next iteration (i.e. epoch) can be derived by calculating the partial derivative weight from the partial derivative Loss**. Or in other words,\n",
    "\n",
    "$$ W = W - \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "$$ V = V - \\frac{\\partial L}{\\partial V}$$\n",
    "\n",
    "This works because...\n",
    "* If $\\frac{\\partial L}{\\partial W}$ is a positive number, then we want to _decrease_ the weight, since increasing the weight would _increase_ our loss. That is exactly what the equation $ W = W - \\frac{\\partial L}{\\partial W}$ does.\n",
    "* Similarly, if $\\frac{\\partial L}{\\partial W}$ is a negative number, then we want to _increase_ the weight, since increasing the weight would _decrease_ our loss. In both cases, the equation $ W = W - \\frac{\\partial L}{\\partial W}$ works.\n",
    "\n",
    "\n",
    "And we can calculate those partial derivatives via the chain rule:\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial W}  $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial V} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial B} * \\frac{\\partial b}{\\partial A} * \\frac{\\partial a}{\\partial V}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# What is a Neural Network: Backwards Propogation\n",
    "The image below is a reminder of where we're going... just reverse the direction of the arrows.\n",
    "![Neural_Intro](img/neural_net_basic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propogating from Loss to Result\n",
    "Recall that the Loss equation is the Mean Squared Error which was:\n",
    "$$ L = l(P) = \\frac{1}{2}(y - P)^2 $$\n",
    "\n",
    "Thus to obtain the partial derivative of the loss over the result (i.e. the rate of change or the gradient of the loss over the result), the equation becomes:\n",
    "$$ \\frac{\\partial l}{\\partial P} = -(y - P)$$\n",
    "\n",
    "This helps us compute the 'accuracy' of our result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propogating over the Activation Function\n",
    "Recall that our Activation Function was the Sigmoid Function, which was:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "That means its derivative is:\n",
    "$$\\sigma'(x) = \\sigma(x) * (1 - \\sigma(x))$$\n",
    "\n",
    "This helps us compute the 'accuracy' of feature 'C' because we now know the rate of how incorrect we were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propogating over the Weights\n",
    "We now need to compute the relationship of the weight & feature which simply means:\n",
    "$$ \\frac{\\partial c}{\\partial W} $$\n",
    "\n",
    "...Or more complicatedly, that means:\n",
    "$$ \\frac{\\partial c}{\\partial W} = \\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} $$\n",
    "                  \n",
    "But because we got Feature C via:\n",
    "$$\n",
    "\\begin{align}\n",
    "C &= \\begin{bmatrix} c_1 \\end{bmatrix} \\\\ \n",
    "&= c(W) \\\\\n",
    "&= w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "That means that each partial derivative of c over w is equivilant to $B^T$:\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}b_1 \\\\\n",
    "                  b_2 \\\\\n",
    "                  b_3 \\\\\n",
    "                  b_4\n",
    "                  \\end{bmatrix} $$\n",
    "\n",
    "This guides us into creating new weights on the next forward propogation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo\n",
    "Now that we know these concepts, take a look at http://playground.tensorflow.org/ for a quick interactive module on Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Other Layer Types\n",
    "What we described is your basic neuron layer, referred to in Keras as a 'Dense' Neural Layer. There are other layer types, and I specifically want to call out two layers. \n",
    "\n",
    "### Batch Normalization\n",
    "Batch Normalization normalizes the data at each neuron within each epoch (i.e. iteration) of the model. It normalizes it by ensuring the mean is close to 0 and the standard deviation is close to 1.\n",
    "\n",
    "While this additional computation adds run time, this normalization process helps data converge much quicker, which should decrease the overall run time, and give you an opportunity to increase the epochs you can use. This helps us get a better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten\n",
    "Not all data sets and models need this, but with our example, we will. The Flatten Layer flattens data into one dimension. This keeps our model consistently work as we feed it both input data (which is four dimensions) and our expected output data (which is two dimensions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Learning Rate & Loss Computation\n",
    "We'll elaborate more about these two later. For now, when we compile our Neural Network, we'll use the `categorical_crossentropy` method to calculate loss and the `adam` optimizer to calculate learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Practical Example: Go to the Keras Notebook\n",
    "Start from the beginning as we build the **World's simplest Neural Network** (I mean it too)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Deep Learning Optimizations\n",
    "Some methods to optimize a Neural Network include:\n",
    "* Learning rate tuning\n",
    "  * Learning rate decay\n",
    "  * Varying learning rates by layer\n",
    "  * Learning rate momentum\n",
    "* Loss Calculation\n",
    "* Preventing Overfitting\n",
    "  * Regularization\n",
    "  * Dropout\n",
    "  * [Not available in Keras yet] Dropconnect\n",
    "* Weight initializations\n",
    "* Different activation functions\n",
    "* Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate + Optimizers\n",
    "The Learning Rate is a number that we multiply the weight by during each iteration -- it determines how much the model will 'listen' to each weight. In the neural network we constructed, we used the `adam` optimizer to fine tune the learning rate.\n",
    "\n",
    "Recall that earlier, I provided the equation to refine the Weight Vector. If we added the learning rate of $\\alpha$, that equation would now look like:\n",
    "$$ W = W - \\alpha * \\frac{\\partial l}{\\partial W}$$\n",
    "\n",
    "Typically speaking, the learning rate should be higher when you're closer to your output and lower when you're closer to your input. This is because when the weight tends to be less informative near the beginning of the model (when you actually provided it data you are certain of) and more informative towards the end of the model.\n",
    "\n",
    "In **Keras**, the Learning Rate is defined by an Optimizer. There are several optimizers in Keras and they all either offer options or implement algorithms that control its decay, momentum, and rate per layer. The list of those optimizers are at: https://keras.io/optimizers/\n",
    "\n",
    "#### Hyper-Quick Summaries of the primary Optimizers\n",
    "Lovingly stolen and summarized from Keras' Documentation and https://datascience.stackexchange.com/questions/10523/guidelines-for-selecting-an-optimizer-for-training-neural-networks\n",
    "1. `AdaGrad` is better for sparse data. It penalizes the learning rate harshly for parameters which are frequently updated but it also gives more learning rate to sparse parameters. `AdaDelta` is similar but it doesn't require an initial learning rate to be set.\n",
    "2. `RMSProp` is better for recurrent neural networks and those are good for data that changes over time.\n",
    "3. `Adam` is overall the best optimizer. It combines the best of Adadelta and RMSprop.\n",
    "4. `Stochastic Gradient Descent` is very basic and is seldom used now because it uses a global learning rate, thus it doesn't work well when the parameters have different scales. It also generally has a hard time escaping the saddle points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Calculation\n",
    "There are several ways to calculate the loss. We used the `categorical_crossentropy` method in our last neural network.\n",
    "\n",
    "In Keras, you can use the Loss Algorithms on https://keras.io/losses/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "Regularization prevents overfitting by ensuring **no weight is a central point of failure** for the entire network. We do this by adding additional terms to larger weights.\n",
    "\n",
    "`L2 Regularization` (otherwise known as Weight Decay) is the most common type. Here, we augment the the error function with the squared magnitude of all weights in the neural network. This in turn ensures that the network uses all the weights rather than honing in on some of the weights.\n",
    "\n",
    "`L1 Regularization` on the other hand is designed to do the opposite -- so only the most important weights are used, which helps the model ignore noise better. This typically performs worse than L2, but this does help at times, especially if you want to know which features are most important. \n",
    "\n",
    "In Keras, you can implement these with the functions at https://keras.io/regularizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Dropout prevents overfitting ensuring **no neuron is a central point of failure** for the entire network. We do this by disconnecting a portion of the neurons (i.e. setting their values to zero) on each forward pass.\n",
    "\n",
    "<img src=\"img/dropout.png\">\n",
    "\n",
    "Based on what we talked about previously, we want to make sure our model learns as much as it can the closer we are to our original input. As such, generally speaking, it makes sense to introduce Dropout on larger networks when some of your final layers are further away from the beginning data.\n",
    "\n",
    "In Keras, you can implement Dropout by following https://keras.io/layers/core/#dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DropConnect\n",
    "Similar to Dropout, but whereas Dropout disabled neurons, here we disable certain weights by setting them to zero.\n",
    "<img src=\"img/drop_connect.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Intialization\n",
    "You can define the initial weights your model will use. Keras lets you do this at: https://keras.io/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions: Relu & the Vanishing Gradient Descent Problem\n",
    "In Keras, a list of Activation Functions are on https://keras.io/activations/ and https://keras.io/layers/advanced-activations/. Attached below is a summarized image of some of the most common Activation Functions from our friends at Wikipedia.\n",
    "\n",
    "<img src='img/Activation.tiff'>\n",
    "\n",
    "As we noted earlier, the Relu Activation Function is one of the most popular activation functions. However, recall that the Relu Activation Function is a linear one.\n",
    "\n",
    "How could a linear transformation work so well? Note that Neural Networks tend to be better with many layers. But the more layers they are, the more 'squished' each neuron gets.\n",
    "\n",
    "<img src='img/grad_descent1.png'> \n",
    "\n",
    "As we add more layers, it becomes more squished. In this example, we're effectively computing S(S(S(S(S(S(S(S(S(S(S(x))))))))))). This becomes silly when we do backwards propogation. If the activation functions don't have poor, training our weights over microscopic differences between datapoints, often with linear derivatives as the backward propogation overly simplifies our stacked activation functions. This is the **Vanishing Gradient Descent Problem**.\n",
    "\n",
    "<img src='img/grad_descent2.png'> \n",
    "\n",
    "The Relu works around this issue though and that's why it's the most frequently used activation function. Both its forward and backwards propogation approach does not 'squish' the data. In return, it helps us amplify what did and didn't work. \n",
    "\n",
    "<img src='img/Activation.tiff'>\n",
    "\n",
    "If you want to learn more about this particular problem, I really like the Jupyter Notebook at: https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Some parameters are considered hyperparameters and you have no choice but to guess what their best values would be. This includes, but definately is not limited to:\n",
    "\n",
    "* The dimensionality/units of your `Dense` Neural Layers (the first parameter in the dense layers)\n",
    "* The percentage of how many neurons you'll `Dropout`\n",
    "* The `Learning Rate` of your `Optimizer`\n",
    "* (When we talk about Convolutional Neural Networks in a second) The size & stride of your `Kernel`\n",
    "\n",
    "There are generally two basic methods you can do this:\n",
    "1. You can create a `Grid Search` where you determine a range of acceptable guesses and create a list of them, usually with a power of 2 step-size such as [8, 16, 32, 64]\n",
    "2. You can do a completely `Random` guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Practical Example: Go to the Keras Notebook\n",
    "Go to the **Demonstrating some concepts in our basic neural network** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Convolutional Neural Networks\n",
    "A convolutional neural network provides several new layer types, in addition to all of the layer types we already talked about.\n",
    "\n",
    "_(All GIFs in this section are obtained from https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59)_\n",
    "\n",
    "\n",
    "\n",
    "### Convolutional Neural Layers\n",
    "The CNN is primarily driven by the `Convolutional Layer`, which effectively is another way to simplify the data.\n",
    "\n",
    "![ConvolutionalLayer](https://cdn-images-1.medium.com/max/1600/1*ZCjPUFrB6eHPRi4eyP6aaA.gif)\n",
    "\n",
    "In the image above, we produce a `convolved feature`/`activation map`/`feature map`. Here are some of the key parts of the image above:\n",
    "* The sliding yellow window is the _Kernel_/_Filter_. This is the multipicative product of weights (denoted in the small red text) and whatever value was originally in that square. These weights change to accomodate what the CNN is learning.\n",
    "* The _Stride_ of the kernel refers to how many 'pixels' it moves in each move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "A **Max Pooling** or **Average Pooling** layer creates a kernel on this convolved feature and completely moves it to seperate regions, selecting either the single highest or the average value across all the values within that kernel.\n",
    "\n",
    "![Pooling](https://cdn-images-1.medium.com/max/800/1*Feiexqhmvh9xMGVVJweXhg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo\n",
    "Now that we know these concepts, let's take a look at an online interactive visualization.\n",
    "http://scs.ryerson.ca/~aharley/vis/conv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
