{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoratical View to Neural Networks\n",
    "\n",
    "**Note, I'll be starting in the Keras Notebook first**\n",
    "\n",
    "I'm more application based when it comes to Data Science -- I'm pretty good with applying the mechanics of it, but I must be honest, I'm not the best with memorizing and remembering the theory.\n",
    "\n",
    "And yet, we'd miss a lot of the magic on why Neural Networks work if we ignore the theory. So let's dive into a super basic premise of what a Neural Network actually is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Why Neural Networks?\n",
    "These first two-to-three sections were lovingly stolen from Seth Weidman, who presented a tutorial at PyData NYC 2017. I encourage you to follow him at www.sethweidman.com. I also encourage you to go to PyData at www.pydata.org\n",
    "\n",
    "So why a Neural Network? Why not Machine Learning?\n",
    "\n",
    "### Visual Example\n",
    "Let's say you have the world's most simplistic dataset which looks like this, and you're trying to classify the colored points.\n",
    "![DataPlot](img/xor.png)\n",
    "\n",
    "There isn't a linear classification or logistic classification method to really classify these data points, without being totally absurd or incorrect -- we cannot draw a line, or even a region, to seperate these two classes. \n",
    "\n",
    "\n",
    "### Math Example\n",
    "Alternatively, let's say you have three points of data, each one containing up to three boolean (i.e. True/False) inputs like this:\n",
    "$$ N(1, 0, 0) = 1 $$\n",
    "$$ N(0, 1, 0) = 1 $$\n",
    "$$ N(1, 1, 0) = 1 $$\n",
    "\n",
    "The third function messes up our ability to use Logistic Regression. In other words, there is no parameter b, w1, w2, or w3 such that:\n",
    "$$N(x_1, x_2, x_3) = \\frac{1}{1 + e^{b + w_1 * x_1 + w_2 * x_2 + w_3 * x_3}}$$\n",
    "\n",
    "\n",
    "### Feature Engineering\n",
    "We could use **Feature Engineering** to solve this. To explain Feature Engineering in a few words, it's manually designing what the input should be. You might try to add some discrimination algorithms or emphasize some key features, to modify your inputs so that its key features become more obvious.\n",
    "\n",
    "Deep Learning -- which includes Neural Networks -- is honestly something like **Architecture Engineering** (don't Google that term, I made it up). In Neural Networks, we're effectively playing around either with the parameters or the architecture of our Neural Network so that it better fits our data. In effect, we end up training the computer to do Feature Engineering on its own.\n",
    "\n",
    "Which solution is better? The new hotness is Deep Learning and letting the computer do the feature engineering rather than us. In practice though, there is a time and place for feature engineering. If you suspect your dataset could be defined with a linear or logistic regression by finetuning some of the inputs, it'll probably be faster from a computation perspective to stick to feature engineering and then slide into machine learning. There's not a good 'one rule' here -- whether you go Feature Engieering + ML or go straight to DL is up to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# What is a Neural Network: Forward Propogation\n",
    "Introducing one of the most classic diagrams in visualizing what a Neural Network looks like:\n",
    "![Neural_Intro](img/neural_net_basic.png)\n",
    "**Disclaimer: I had a typo on this diagram. The second set of weights on the right should be 'w', not 'v'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Applying Weights\n",
    "The first step most Neural Networks take is to take the inputs, multiply it by some weight, to obtain a feature.\n",
    "\n",
    "$$ a_1 = x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} $$\n",
    "$$ a_2 = x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} $$\n",
    "$$ a_3 = x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} $$\n",
    "$$ a_4 = x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} $$\n",
    "\n",
    "In the first pass, these weights are probably something absurdly silly (like all 1s, all 0s, etc.). Overtime, they get refined into more helpful units, and we'll talk more about this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "The second step is passing the feature through an **Activation Function**. This is important because if we stuck with the above functions through the entire process, we'd stay Linear and create a Linear Transformation. Chances are, you're here because you want a non-linear transformation and this is where Activation Function comes in.\n",
    "\n",
    "There are many 'premade' Activation Functions and many Activation Functions that are being researched. Here is an example of an activation function that you will almost certainly use as the last function in any classification problem. This is the **Sigmoid Function** which maps numbers from 0 to 1.\n",
    "\n",
    "$$ B = \\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Here's a list of all the possible Activation Functions. Later, we'll talk about the **Relu Function** which is arguably the most popular activation function, although the reason it works so well will probably surprise you.\n",
    "\n",
    "<img src='img/Activation.tiff'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "The Loss is the difference between the estimated result and actual result. Mathematically, this is often the **Mean Squared Error Loss**. The lower the loss is, the better.\n",
    "\n",
    "$$ L = \\frac{1}{2}(y - P)^2 $$\n",
    "\n",
    "There are other Loss Functions you can use and we'll talk more about them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# What is a Neural Network: Forwards to Backwards\n",
    "We continue these algorithms noted above, repeating them until we reach the end of the diagram I showed you before.\n",
    "\n",
    "But you might ask how the weights change -- because by default, they start at silly & unhelpful states. These weights do refine in a process we call `Backpropogation` and/or `Backwards Propogation` (depending who you talk to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does Backpropogation work?\n",
    "Recall that a Neural Network is effectively, some function that takes the inputs of the last function and passes its output to the next function.\n",
    "\n",
    "\\begin{align}\n",
    "A &= a(x, V) \\\\\n",
    "B &= b(A) \\\\\n",
    "C &= c(B, W) \\\\\n",
    "P &= p(C) \\\\\n",
    "L &= l(P)\n",
    "\\end{align}\n",
    "\n",
    "Because these equations are so linked, we can combine them in one line such as below:\n",
    "\n",
    "$$ L = l(p(c(b(a(x, V)), W))) $$\n",
    "\n",
    "Notice that both Weights (W & V) are related to the functions which create the Neural Network. This implies that **the calculation of those weights in the next iteration (i.e. epoch) can be derived by calculating the partial derivative weight from the partial derivative Loss**. Or in other words,\n",
    "\n",
    "$$ W = W - \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "$$ V = V - \\frac{\\partial L}{\\partial V}$$\n",
    "\n",
    "This works because...\n",
    "* If $\\frac{\\partial L}{\\partial W}$ is a positive number, then we want to _decrease_ the weight, since increasing the weight would _increase_ our loss. That is exactly what the equation $ W = W - \\frac{\\partial L}{\\partial W}$ does.\n",
    "* Similarly, if $\\frac{\\partial L}{\\partial W}$ is a negative number, then we want to _increase_ the weight, since increasing the weight would _decrease_ our loss. In both cases, the equation $ W = W - \\frac{\\partial L}{\\partial W}$ works.\n",
    "\n",
    "\n",
    "And we can calculate those partial derivatives via the chain rule:\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial W}  $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial V} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial B} * \\frac{\\partial b}{\\partial A} * \\frac{\\partial a}{\\partial V}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# What is a Neural Network: Backwards Propogation\n",
    "The image below is a reminder of where we're going... just reverse the direction of the arrows.\n",
    "![Neural_Intro](img/neural_net_basic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propogating from Loss to Result\n",
    "Recall that the Loss equation is the Mean Squared Error which was:\n",
    "$$ L = l(P) = \\frac{1}{2}(y - P)^2 $$\n",
    "\n",
    "Thus to obtain the partial derivative of the loss over the result (i.e. the rate of change or the gradient of the loss over the result), the equation becomes:\n",
    "$$ \\frac{\\partial l}{\\partial P} = -(y - P)$$\n",
    "\n",
    "This helps us compute the 'accuracy' of our result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propogating over the Activation Function\n",
    "Recall that our Activation Function was the Sigmoid Function, which was:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "That means its derivative is:\n",
    "$$\\sigma'(x) = \\sigma(x) * (1 - \\sigma(x))$$\n",
    "\n",
    "This helps us compute the 'accuracy' of feature 'C' because we now know the rate of how incorrect we were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propogating over the Weights\n",
    "We now need to compute the relationship of the weight & feature which simply means:\n",
    "$$ \\frac{\\partial c}{\\partial W} $$\n",
    "\n",
    "...Or more complicatedly, that means:\n",
    "$$ \\frac{\\partial c}{\\partial W} = \\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} $$\n",
    "                  \n",
    "But because we got Feature C via:\n",
    "$$\n",
    "\\begin{align}\n",
    "C &= \\begin{bmatrix} c_1 \\end{bmatrix} \\\\ \n",
    "&= c(W) \\\\\n",
    "&= w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "That means that each partial derivative of c over w is equivilant to $B^T$:\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}b_1 \\\\\n",
    "                  b_2 \\\\\n",
    "                  b_3 \\\\\n",
    "                  b_4\n",
    "                  \\end{bmatrix} $$\n",
    "\n",
    "This guides us into creating new weights on the next forward propogation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo\n",
    "Now that we know these concepts, take a look at http://playground.tensorflow.org/ for a quick interactive module on Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Other Layer Types\n",
    "What we described is your basic neuron layer, referred to in Keras as a 'Dense' Neural Layer. There are other layer types, and I specifically want to call out two layers. \n",
    "\n",
    "### Batch Normalization\n",
    "Batch Normalization normalizes the data at each neuron within each epoch (i.e. iteration) of the model. It normalizes it by ensuring the mean is close to 0 and the standard deviation is close to 1.\n",
    "\n",
    "While this additional computation adds run time, this normalization process helps data converge much quicker, which should decrease the overall run time, and give you an opportunity to increase the epochs you can use. This helps us get a better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten\n",
    "Not all data sets and models need this, but with our example, we will. The Flatten Layer flattens data into one dimension. This keeps our model consistently work as we feed it both input data (which is four dimensions) and our expected output data (which is two dimensions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Learning Rate & Loss Computation\n",
    "We'll elaborate more about these two later. For now, when we compile our Neural Network, we'll use the `categorical_crossentropy` method to calculate loss and the `adam` optimizer to calculate learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Practical Example: Go to the Keras Notebook\n",
    "Start from the beginning as we build the **World's simplest Neural Network** (I mean it too)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Deep Learning Optimizations\n",
    "Some methods to optimize a Neural Network include:\n",
    "* Learning rate tuning\n",
    "  * Learning rate decay\n",
    "  * Varying learning rates by layer\n",
    "  * Learning rate momentum\n",
    "* Loss Calculation\n",
    "* Preventing Overfitting\n",
    "  * Regularization\n",
    "  * Dropout\n",
    "  * [Not available in Keras yet] Dropconnect\n",
    "* Weight initializations\n",
    "* Different activation functions\n",
    "* Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate + Optimizers\n",
    "The Learning Rate is a number that we multiply the weight by during each backpropogation epoch -- it determines how much the model will 'listen' to each weight. In the neural network we constructed, we used the `adam` optimizer to fine tune the learning rate.\n",
    "\n",
    "Recall that earlier, I provided the equation to refine the Weight Vector. If we added the learning rate of $\\alpha$, that equation would now look like:\n",
    "$$ W = W - \\alpha * \\frac{\\partial l}{\\partial W}$$\n",
    "\n",
    "* When the weight is closer to the input, we want the learning rate to be lower, so that we pay more attention to the input data (and less about the weight).\n",
    "* When the weight is closer to the output, we want the learning rate to be higher, so that we pay more attention to how the weight changed, because by now, we're getting 'echos' of what the input used to be.\n",
    "\n",
    "In **Keras**, the Learning Rate is defined by an Optimizer. There are several optimizers in Keras and they all either offer options or implement algorithms that control its decay, momentum, and rate per layer. The list of those optimizers are at: https://keras.io/optimizers/\n",
    "\n",
    "#### Hyper-Quick Summaries of the primary Optimizers\n",
    "Lovingly stolen and summarized from Keras' Documentation and https://datascience.stackexchange.com/questions/10523/guidelines-for-selecting-an-optimizer-for-training-neural-networks\n",
    "1. `AdaGrad` is better for sparse data. It penalizes the learning rate harshly for parameters which are frequently updated but it also gives more learning rate to sparse parameters. `AdaDelta` is similar but it doesn't require an initial learning rate to be set.\n",
    "2. `RMSProp` is better for recurrent neural networks and those are good for data that changes over time.\n",
    "3. `Adam` is overall the best optimizer. It combines the best of Adadelta and RMSprop.\n",
    "4. `Stochastic Gradient Descent` is very basic and is seldom used now because it uses a global learning rate, thus it doesn't work well when the parameters have different scales. It also generally has a hard time escaping the saddle points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Calculation\n",
    "There are several ways to calculate the loss. We used the `categorical_crossentropy` method in our last neural network.\n",
    "\n",
    "In Keras, you can use the Loss Algorithms on https://keras.io/losses/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "Regularization prevents overfitting by ensuring **no weight is a central point of failure** for the entire network. We do this by adding additional terms to larger weights.\n",
    "\n",
    "`L2 Regularization` (otherwise known as Weight Decay) is the most common type. Here, we augment the the error function with the squared magnitude of all weights in the neural network. This in turn ensures that the network uses all the weights rather than honing in on some of the weights.\n",
    "\n",
    "`L1 Regularization` on the other hand is designed to do the opposite -- so only the most important weights are used, which helps the model ignore noise better. This typically performs worse than L2, but this does help at times, especially if you want to know which features are most important. \n",
    "\n",
    "In Keras, you can implement these with the functions at https://keras.io/regularizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Dropout prevents overfitting ensuring **no neuron is a central point of failure** for the entire network. We do this by disconnecting a portion of the neurons (i.e. setting their values to zero) on each forward pass.\n",
    "\n",
    "<img src=\"img/dropout.png\">\n",
    "\n",
    "Based on what we talked about previously, we want to make sure our model learns as much as it can the closer we are to our original input. As such, generally speaking, it makes sense to introduce Dropout on larger networks when some of your final layers are further away from the beginning data.\n",
    "\n",
    "In Keras, you can implement Dropout by following https://keras.io/layers/core/#dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DropConnect\n",
    "Similar to Dropout, but whereas Dropout disabled neurons, here we disable certain weights by setting them to zero.\n",
    "<img src=\"img/drop_connect.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Intialization\n",
    "You can define the initial weights your model will use. Keras lets you do this at: https://keras.io/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions: Relu & the Vanishing Gradient Descent Problem\n",
    "In Keras, a list of Activation Functions are on https://keras.io/activations/ and https://keras.io/layers/advanced-activations/. Attached below is a summarized image of some of the most common Activation Functions from our friends at Wikipedia.\n",
    "\n",
    "<img src='img/Activation.tiff'>\n",
    "\n",
    "As we noted earlier, the Relu Activation Function is one of the most popular activation functions. However, recall that the Relu Activation Function is a linear one.\n",
    "\n",
    "How could a linear transformation work so well? Note that Neural Networks tend to be better with many layers. But the more layers they are, the more 'squished' each neuron gets, if we pick an activation function that forces our values to be within a range (such as tan)\n",
    "\n",
    "<img src='img/grad_descent1.png'> \n",
    "\n",
    "As we add more layers, it becomes more squished. In this example, we're effectively computing S(S(S(S(S(S(S(S(S(S(S(x))))))))))). This becomes silly when we do backwards propogation. If the activation functions don't have poor, training our weights over microscopic differences between datapoints, often with linear derivatives. This overly simplifies our stacked activation functions and results in slow (if even effective) weight refinements. This is the **Vanishing Gradient Descent Problem**.\n",
    "\n",
    "<img src='img/grad_descent2.png'> \n",
    "\n",
    "The Relu works around this issue though because it's range is effectively 0 to the max(x). Also, because its derivative is not linear persay. Both its forward and backwards propogation approach does not 'squish' the data and this means we get better data as a result.\n",
    "\n",
    "<img src='img/Activation.tiff'>\n",
    "\n",
    "If you want to learn more about this particular problem, I really like the Jupyter Notebook at: https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Some parameters are considered hyperparameters and you have no choice but to guess what their best values would be. This includes, but definately is not limited to:\n",
    "\n",
    "* The dimensionality/units of your `Dense` Neural Layers (the first parameter in the dense layers)\n",
    "* The percentage of how many neurons you'll `Dropout`\n",
    "* The `Learning Rate` of your `Optimizer`\n",
    "* (When we talk about Convolutional Neural Networks in a second) The size & stride of your `Kernel`\n",
    "\n",
    "There are generally two basic methods you can do this:\n",
    "1. You can create a `Grid Search` where you determine a range of acceptable guesses and create a list of them, usually with a power of 2 step-size such as [8, 16, 32, 64]\n",
    "2. You can do a completely `Random` guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Practical Example: Go to the Keras Notebook\n",
    "Go to the **Demonstrating some concepts in our basic neural network** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Convolutional Neural Networks\n",
    "**Convolutional Neural Networks are typically used to do image recognition and/or classification tasks**. They work best when they're analyzing data that is structured -- that is, a particular data point has some relation with the other data points surrounding it.\n",
    "\n",
    "They add two new primary layer types which help extract features from its input data. Those layers are `Convolutional Layers` and `Pooling Layers`.\n",
    "\n",
    "_(All GIFs in this section are obtained from https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59)_\n",
    "\n",
    "\n",
    "\n",
    "### Convolutional Neural Layers\n",
    "The CNN is primarily driven by the `Convolutional Layer`, which effectively is another way to simplify the data.\n",
    "\n",
    "![ConvolutionalLayer](https://cdn-images-1.medium.com/max/1600/1*ZCjPUFrB6eHPRi4eyP6aaA.gif)\n",
    "\n",
    "In the image above, we produce a `convolved feature`/`activation map`/`feature map`. Here are some of the key parts of the image above:\n",
    "* The sliding yellow window is the _Kernel_/_Filter_. This is the multipicative product of weights (denoted in the small red text) and whatever value was originally in that square. These weights change to accomodate what the CNN is learning.\n",
    "* The _Stride_ of the kernel refers to how many 'pixels' it moves in each move\n",
    "\n",
    "The weights are applied just like a standard neural network. So for the first nine pixels in the upper right, we're computing: `(1*1)+(1*0)+(1*1)+(0*0)+(1*1)+(1*0)+(0*1)+(0*0)+(1*1) = 4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "A **Max Pooling** or **Average Pooling** layer creates a kernel on this convolved feature and completely moves it to seperate regions, selecting either the single highest or the average value across all the values within that kernel.\n",
    "\n",
    "![Pooling](https://cdn-images-1.medium.com/max/800/1*Feiexqhmvh9xMGVVJweXhg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lay of the Land\n",
    "You would typically combine these two layers in the beginning, often with several passes. These layers would then feed data into the `Dense` or `Fully Connected` layers we created in the last basic neural network.\n",
    "\n",
    "<img src=\"img/CNNArchitecture.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo\n",
    "Now that we know these concepts, let's take a look at an online interactive visualization.\n",
    "http://scs.ryerson.ca/~aharley/vis/conv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Practical Example: Go to the Keras Notebook\n",
    "Go to the **Convolutional Neural Networks** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "# CNN Best Practices \n",
    "CNNs are a very new field. There aren't many 'best practices' and researchers are trying to discover how to develop convolutional neural networks for common datasets (such as MNIST -- not to be confused with our Fashion MNIST dataset), let alone other datasets.\n",
    "\n",
    "If you're remotely interested in CNNs, I strongly urge you to reach some of the research papers from the most famous CNNs to understand why they exist. The models get better as the papers get newer, but each paper really builds off of the prior model.\n",
    "\n",
    "Here's a quick overview into all of these models: https://medium.com/@siddharthdas_32104/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5. All of these models have research papers which I believe are all free & open source.\n",
    "\n",
    "Keras does have these models already pre-made for use and its documentation is at https://keras.io/applications/. I would urge you to be careful with these though, becuase these models were designed for the dataset that they were researched with, which probably isn't your dataset. It's better to understand how these models work and see how it might align to your dataset rather than use it outright. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second place CNN: ResNets\n",
    "ResNets (2015) are one of the newest and best CNNs out there. Its research paper is: https://arxiv.org/pdf/1512.03385.pdf.\n",
    "\n",
    "It's claim to fame is its strange architecture. In a ResNet, data can pass into the next filter directly (which is normal) *and* via shortcuts that skip a few layers along the way (which is not normal). You will have to get creative with Keras (or any other package for that matter) to code this up.\n",
    "\n",
    "<img src=\"img/ResNets.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first place CNN: DenseNets\n",
    "The problem with ResNets is that the computation can get intense quickly. Researchers realized that many layers were not needed and some of it was redundant. They asked if they can get by reusing features. That is, by feeding the concated conclusions of the prior dense/neural layers in a particular subset to the next dense/neural layer.\n",
    "\n",
    "<img src=\"img/DenseNets.png\"/>\n",
    "\n",
    "Its research paper is: https://arxiv.org/pdf/1608.06993.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Recurrent Neural Networks\n",
    "**Recurring Neural Networks are typically used when the sequence of your data is important in how your model performs**. Examples include parsing language (because each word influences the grammatical construct of the next word) or how something might fly in the air over time.\n",
    "\n",
    "Recall that in the World's Simplest Neural Network & Convolutional Neural Network, it really doesn't matter in what order our images come in. A RNN twists this around by taking the output from each layer of neurons and passing it into the activation functions for the next data sample to come in.\n",
    "\n",
    "<img src='img/RNN-unrolled.png'/>\n",
    "Image stolen from http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "### The Hidden Layers\n",
    "The big takeaway is that the neuron computing the Activation Function -- known in RNNs as the `Hidden Layer` -- now requires these inputs:\n",
    "* The current time step's input & weights\n",
    "* The prior time step's input & weights\n",
    "  * Which in turn, was influenced by prior time steps.\n",
    "  \n",
    "Here's another GIF to help explain this (lovingly stolen from https://deeplearning4j.org/lstm.html). Here, `x` is the input data, `w` is the weight, `a` is the activation function inside the hidden layer (i.e. neuron), and `b` is the output for said input data.\n",
    "\n",
    "<img src='img/RNN.gif'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation & the Vanishing Gradient Problem\n",
    "Recall that the Vanishing Gradient Problem, for typical neural networks, exists after we add many Dense Layers. That the first few layers might impact the output greatly, but results will soon begin to stagnate with additional layers, and this makes refining our weights harder during backpropogation.\n",
    "\n",
    "In CNNs, we came across two conclusions:\n",
    "1. Using `ReLU` activation functions is ideal, because during backpropogation, the derivative of ReLU does not result in a linear function.\n",
    "2. There is a point where we can have too many layers, so that our outputs begin to stagnate.\n",
    "\n",
    "RNNs are similiar, but in a slightly different way. Let's say we are parsing a sentence in an RNN. The words used depends on the other words in the sentence, but it does not necessarily depend on the words immediately before or after.\n",
    "\n",
    "As discussed, RNNs passes data from each data sample to the next data sample it predicts. This passing of data creates an ever increasing chain rule in size.\n",
    "\n",
    "<img src='img/RNN_Gradient.png'/>\n",
    "\n",
    "**In Neural Networks + CNNs, our results started to stagnate as we add more layers. In RNNs, our results will start to stagnate as this sentence (or whatever data set) gets longer.** This is because each word adds yet another 'dimension' to our calculations. And our weights adapt less efficiently during backpropogation for activation functions that do not derive well, similar to Neural Networks + CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Two Types of RNNs\n",
    "To address the Vanishing Gradient Problem, two new types of RNNs exist nowadays. Both treat the Activation Function -- the hidden layer -- like a black box and utilizes different computation algorithms.\n",
    "\n",
    "### Long Short-Term Memory (LSTM) \n",
    "Without an LSTM, our Hidden Layer would compute one activation function like this.\n",
    "<img src='img/RNN-PreLSTM.png'/>\n",
    "\n",
    "I make no doubt that http://colah.github.io/posts/2015-08-Understanding-LSTMs/ explained this process far better than I could ever dream of communicating. But here's a quick stab into what's going on. \n",
    "\n",
    "<img src='img/RNN-LSTM.png'/>\n",
    "\n",
    "Each LSTM Layer takes in:\n",
    "1. The input data from the current time step (x)\n",
    "2. The input data from the prior time step (h)\n",
    "3. The `Cell State` data from the prior time step (C). This helps the model determine if it should rely on the prior time step's data or if it needs to think differently.\n",
    "\n",
    "Going from left to right:\n",
    "* The first branch determines **if we should use the data from the last time step**.\n",
    "  * We compute the sigmoid function to produce a 1 (Yay) or 0 (Boo) over the current timestep's & former timestep's data. This is multiplied against the former cell state's output.\n",
    "  * For example, if we're parsing a sentence and we introduced a new subject midway, it would behoove our model to ignore the gender/traits of the prior subject.\n",
    "* The second & third branch conducts the **update conclusion from the first branch**.\n",
    "  * We compute the sigmoid function (returning 1 or 0) and tanh function (returning from a range of -1 to 1) between our current timestep & former timestep.\n",
    "  * We multiply those values together and add it to the cell state.\n",
    "  * Using our prior example, we would actually compute the effects of ignoring our former subject and focusing on this new subject here.\n",
    "* The final branch determines **which part of the input should be outputted**\n",
    "  * We compute the sigmoid function on our current and former timestep.\n",
    "    * This becomes our output\n",
    "    * This also becomes multiplied by a tanh activation function and gets appended to our cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit (GRU)\n",
    "This is similar to an LSTM, but it's a bit more optimized by combining similar functions together and introducing `Peepholes` where our prior cell state becomes used as opposed to using the prior timestep's data.\n",
    "\n",
    "<img src='img/RNN-GRU.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
