{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoratical View to Neural Networks\n",
    "I'm more application based when it comes to Data Science -- I'm pretty good with applying the mechanics of it, but I must be honest, I'm not the best with memorizing and remembering the theory.\n",
    "\n",
    "And yet, we'd miss a lot of the magic on why Neural Networks work if we ignore the theory. So let's dive into a super basic premise of what a Neural Network actually is.\n",
    "\n",
    "I would be a liar if I said I did this entire Notebook by myself. A lot of it -- especially the images -- were taken from Seth Weidman, who presented a tutorial at PyData NYC 2017. I encourage you to follow him at www.sethweidman.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Why Neural Networks?\n",
    "So why a Neural Network? Why not Machine Learning?\n",
    "\n",
    "### Visual Example\n",
    "Let's say you have the world's most simplistic dataset which looks like this, and you're trying to classify the colored points.\n",
    "![DataPlot](img/xor.png)\n",
    "\n",
    "There isn't a linear classification or logistic classification method to really classify these plots, without being totally absurd or incorrect -- we cannot draw a line, or even a region, to seperate these two classes. \n",
    "\n",
    "\n",
    "### Math Example\n",
    "Alternatively, let's say you have three points of data, each one containing up to three boolean (i.e. True/False) inputs like this:\n",
    "$$ N(1, 0, 0) = 1 $$\n",
    "$$ N(0, 1, 0) = 1 $$\n",
    "$$ N(1, 1, 0) = 1 $$\n",
    "\n",
    "The third function messes up our ability to use Logistic Regression. In other words, there is no parameter b, w1, w2, or w3 such that:\n",
    "$$N(x_1, x_2, x_3) = \\frac{1}{1 + e^{b + w_1 * x_1 + w_2 * x_2 + w_3 * x_3}}$$\n",
    "\n",
    "\n",
    "### Feature Engineering\n",
    "We could use **Feature Engineering** to solve this. To explain Feature Engineering in a few words, it's manually designing what the input should be. You might try to add some discrimination algorithms or emphasize some key features, to modify your inputs so that its key features become more obvious.\n",
    "\n",
    "Deep Learning -- which includes Neural Networks -- is honestly something like **Architecture Engineering** (don't Google that term, I made it up). In Neural Networks, we're effectively playing around either with the parameters or the architecture of our Neural Network so that it better fits our data. In effect, we end up training the computer to do Feature Engineering on its own.\n",
    "\n",
    "Which solution is better? The new hotness is Deep Learning and letting the computer do the feature engineering rather than us. In practice though, there is a time and place for feature engineering. If you suspect your dataset could be defined with a linear or logistic regression by finetuning some of the inputs, it'll probably be faster from a computation perspective to stick to feature engineering and then slide into machine learning. There's not a good 'one rule' here -- whether you go Feature Engieering + ML or go straight to DL is up to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# What is a Neural Network: Forward Propogation\n",
    "Introducing one of the most classic diagrams in visualizing what a Neural Network looks like:\n",
    "![Neural_Intro](img/neural_net_basic.png)\n",
    "**Disclaimer: I had a typo on this diagram. The second set of weights on the right should be 'w', not 'v'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Weights\n",
    "The first step most Neural Networks take is to take the inputs, multiply it by some weight, to obtain a feature.\n",
    "\n",
    "$$ a_1 = x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} $$\n",
    "$$ a_2 = x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} $$\n",
    "$$ a_3 = x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} $$\n",
    "$$ a_4 = x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} $$\n",
    "\n",
    "I'll talk about how these weights are refined later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "The second step is passing the feature through an **Activation Function**. This is important because if we stuck with the above functions through the entire process, we'd stay Linear and create a Linear Transformation. Chances are, you're here because you want a non-linear transformation and this is where Activation Function comes in.\n",
    "\n",
    "There are many 'premade' Activation Functions and many Activation Functions that are being researched. Here is (what I believe is) the most important Activation Function, used at the end of most classification problems. This is the **Sigmoid Function** which maps numbers from 0 to 1.\n",
    "\n",
    "$$ B = \\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "If we're following the above diagram, the Weights & Activation Function approach is replicated again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "The Loss is the difference between the estimated result and actual result. Mathematically, this is the **Mean Squared Error Loss** and the lower it is, the better.\n",
    "\n",
    "$$ L = \\frac{1}{2}(y - P)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refining Weights in the next Iteration\n",
    "Recall that a Neural Network is effectively, some function that takes the inputs of the last function and passes its output to the next function.\n",
    "\n",
    "\\begin{align}\n",
    "A &= a(x, V) \\\\\n",
    "B &= b(A) \\\\\n",
    "C &= c(B, W) \\\\\n",
    "P &= p(C) \\\\\n",
    "L &= l(P)\n",
    "\\end{align}\n",
    "\n",
    "Because these equations are so linked, we can combine them in one line such as below:\n",
    "\n",
    "$$ L = l(p(c(b(a(x, V)), W))) $$\n",
    "\n",
    "Notice that both Weights (W & V) are related to the functions which create the Neural Network. This implies that the calculation of those weights in the next iterations can be derived by calculating the partial derivative weight from the partial derivative Loss. Or in other words,\n",
    "\n",
    "$$ W = W - \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "$$ V = V - \\frac{\\partial L}{\\partial V}$$\n",
    "\n",
    "This works because...\n",
    "* If $\\frac{\\partial L}{\\partial W}$ is a positive number, then we want to _decrease_ the weight, since increasing the weight would _increase_ our loss. That is exactly what the equation $ W = W - \\frac{\\partial L}{\\partial W}$ does.\n",
    "* Similarly, if $\\frac{\\partial L}{\\partial W}$ is a negative number, then we want to _increase_ the weight, since increasing the weight would _decrease_ our loss. In both cases, the equation $ W = W - \\frac{\\partial L}{\\partial W}$ works.\n",
    "\n",
    "\n",
    "And we can calculate those partial derivatives via the chain rule:\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial W}  $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial V} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial B} * \\frac{\\partial b}{\\partial A} * \\frac{\\partial a}{\\partial V}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# What is a Neural Network: Backwards Propogation\n",
    "We finish this iteration of the neural network by computing backwards, from the loss, towards the original input. We can do this because every function is connected to one another and we do this to help train our weights, using information we now know about how accurate or inaccurate we were.\n",
    "\n",
    "The image below is a reminder of where we're going... just reverse the direction of the arrows.\n",
    "![Neural_Intro](img/neural_net_basic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propogating from Loss to Result\n",
    "Recall that the Loss equation is the Mean Squared Error which was:\n",
    "$$ L = l(P) = \\frac{1}{2}(y - P)^2 $$\n",
    "\n",
    "Thus to obtain the partial derivative of the loss over the result, the equation becomes:\n",
    "$$ \\frac{\\partial l}{\\partial P} = -(y - P)$$\n",
    "\n",
    "This takes us back to the Result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propogating over the Activation Function\n",
    "Recall that our Activation Function was the Sigmoid Function, which was:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "That means its derivative is:\n",
    "$$\\sigma'(x) = \\sigma(x) * (1 - \\sigma(x))$$\n",
    "\n",
    "This takes us back to Feature C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propogating over the Weights\n",
    "We now need to compute the relationship of the weight & feature which simply means:\n",
    "$$ \\frac{\\partial c}{\\partial W} $$\n",
    "\n",
    "...Or more complicatedly, that means:\n",
    "$$ \\frac{\\partial c}{\\partial W} = \\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} $$\n",
    "                  \n",
    "But because we got Feature C via:\n",
    "$$\n",
    "\\begin{align}\n",
    "C &= \\begin{bmatrix} c_1 \\end{bmatrix} \\\\ \n",
    "&= c(W) \\\\\n",
    "&= w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "That means that each partial derivative of c over w is equivilant to $B^T$:\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}b_1 \\\\\n",
    "                  b_2 \\\\\n",
    "                  b_3 \\\\\n",
    "                  b_4\n",
    "                  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Deep Learning Optimizations\n",
    "Some methods to optimize a Neural Network include:\n",
    "* Learning rate tuning\n",
    "  * Learning rate decay\n",
    "  * Varying learning rates by layer\n",
    "  * Learning rate momentum\n",
    "* Dropout\n",
    "* Dropconnect\n",
    "* Weight initializations\n",
    "* Different activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "The Learning Rate is a number that we multiply the weight by during each iteration. \n",
    "\n",
    "Recall that earlier, I provided the equation to refine the Weight Vector. If we added the learning rate of $\\alpha$, that equation would now look like:\n",
    "$$ W = W - \\alpha * \\frac{\\partial l}{\\partial W}$$\n",
    "\n",
    "Typically speaking, the learning rate should be higher when you're closer to your output and lower when you're closer to your input. This is because when the weight tends to be less informative near the beginning of the model (when you actually provided it data you are certain of) and more informative towards the end of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Dropout prevents overfitting by disconnecting a portion of the neurons -- setting their values to zero -- on eaach forward pass. \n",
    "\n",
    "<img src=\"img/dropout.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DropConnect\n",
    "Similar to Dropout, but whereas Dropout disabled neurons, here we disable certain weights by setting them to zero.\n",
    "<img src=\"img/drop_connect.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
