{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi There\n",
    "I'm Matthew Widjaja, I graduated from the Data Science program this year but I've been at Stockton since 2010, starting with the Computational Science BS program, which eventually morphed into this very program. \n",
    "\n",
    "Today, we'll be working with these technical concepts. If you do not have these on your laptop, I encourage you to grab them now.\n",
    "\n",
    "1. Anaconda 5 (I'm using 5.0.1 but 5.1.0 works too)\n",
    "2. Keras\n",
    "  1. `conda install keras`\n",
    "3. TensorFlow (you can use Theano or Microsoft CNTK but you'll have to make those modifications in keras.json)\n",
    "  1. `conda install tensorflow`\n",
    "\n",
    "You can also grab these Jupyter Notebooks and read along or play along from your computer. They're available at https://github.com/mwidjaja1/Neural-Network-Basics. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Neural Network in Keras\n",
    "\n",
    "All good projects start with a Dataset and today, that's the **Fashion-MNIST** dataset. It has 60,000 training samples & 10,000 test samples of grayscale images, each 28x28 pixels. It's available at: https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "<img src='img/fashionmnist.png'>\n",
    "\n",
    "Each clothing article is labeled into 10 categories which are...\n",
    "\n",
    "0. T-Shirt/Top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle Boot\n",
    "\n",
    "And as a note, the dataset is zeros-indexed so T-Shirt/Top is actually #0. Jupyter Notebook's lists are not zeros-indexed.\n",
    "\n",
    "If you go to the Github page, the authors of this dataset make a pretty convincing argument why this dataset is better than the far more popular MNIST dataset, which has images containing handwritten numerical digits, and is used around the world to test new networks. In summary, the MNIST dataset has been used so much, it's not really challenging anymore to predict it -- even to the point that normal machine learning models can figure it out.\n",
    "\n",
    "The authors also tested various machine learning models against this dataset at http://fashion-mnist.s3-website.eu-central-1.amazonaws.com. The summary is the very best model was an SVC with C=10 & Kernel='Poly', with an accuracy of 0.897 and 1:12:39 training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Pre-Requisites\n",
    "To do this, I assume you have the following:\n",
    "* Anaconda 5.0.1 (This is a Python Distribution with several scientific libraries built in)\n",
    "* Keras\n",
    "* TensorFlow\n",
    "* [Optional] Theano\n",
    "\n",
    "I assume you're running with the latest version of all of Anaconda's libraries. To upgrade Anaconda, you'd run `conda upgrade --all` in your terminal. If you're using Windows... God have mercy on your soul.\n",
    "\n",
    "To install Keras, TensorFlow, & Theano: Run `conda install keras tensorflow theano` in your terminal.\n",
    "\n",
    "*NVIDIA GPU Note:* If you have an NVIDIA GPU on your computer, this specific command will not give you a GPU enabled version of TensorFlow. To get a GPU enabled version, you can follow the instructions at https://www.tensorflow.org/install/, but I must admit that I ran into some issues integrating it with Anaconda (correctly). I personally got my GPU enabled copy of TensorFlow from https://github.com/nathanielatom/tensorflow, which Nathaniel already compiled for all the different operating systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Keras?\n",
    "**Keras** is a high level Python API that allows for fast prototyping of Convolutional and/or Recurrent Neural Networks. Because it integrates with TensorFlow, Theano, and Microsoft CNTK, it lets us test each of those three computation libraries because different libraries could perform better in different models. \n",
    "\n",
    "By default, Keras is ready to go to use TensorFlow. If you wanted to use a different library and/or wanted to use your GPU, you'll need to make a file at `~/.keras/keras.json` (where ~ = your home dir). For example, mine looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"image_data_format\": \"channels_last\",\n",
    "    \"backend\": \"tensorflow\",\n",
    "    \"device\": \"cuda\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TensorFlow, Theano, or Microsoft CNTK.\n",
    "**TensorFlow** is a library with algorithms and concepts to accelerate the types of computation often used in Deep Learning. TensorFlow is Google's solution. It's what we'll use today becuase it's Mac + Linux + Windows compatible and is generally faster on CPUs, which I assume most of us are using today. \n",
    "\n",
    "This isn't to discount the other libraries though. **Theano** is a popular library although it has been discontinued. Originally created by the academic community, it is the oldest library by far, and I would argue is a more 'stable' library given how TensorFlow & Microsoft CNTK changes quickly. While it won't recieve any new development as academic knowledge increases, it definately is still honored and well respected as the original computation library.\n",
    "\n",
    "**Microsoft CNTK** is extremely new and is in a lot of active development. While it's faster & more accurate than TensorFlow, it also has a smaller community, partially due to its recent creation & feelings about Microsoft. While we won't use this today, it's something worth keeping an eye on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Loading up the Data\n",
    "Keras already has a built in function to load the Fashion MNIST dataset so we'll use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Data\n",
    "`y_train` and `y_test` are Numpy Arrays with the correct classification value of each image. These are one dimensional arrays with a value for each sample in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train is a Numpy Array which is shaped and looks like this\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are the 'classes' that are valid for each integer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of valid classes\n",
    "classes = {0: 'T-Shirt/top',\n",
    "           1: 'Trouser',\n",
    "           2: 'Pullover',\n",
    "           3: 'Dress',\n",
    "           4: 'Coat',\n",
    "           5: 'Sandal',\n",
    "           6: 'Shirt',\n",
    "           7: 'Sneaker',\n",
    "           8: 'Bag',\n",
    "           9: 'Ankle boot'}\n",
    "classes_list = list(classes.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reshape this dataset. Right now, this is a one dimensional dataset with an entry for each image. Keras + TensorFlow expects this shape to `(qtyOfSamples, qtyOfClasses, qtyOfClasses)`. Keras comes with a built in Numpy Utilities library which can auto-generate this array so that all values per each sample is 0, except for the single value which correctly distinguishes which class applies to the given sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data\n",
    "`x_train` and `x_test` are Numpy Arrays which have the inputs for the training and testing data set. Each set contains either 60,000 samples or 10,000 samples and each sample is 28x28 pixels. Because each sample is a grayscale image, this is a binary dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x_train is a Numpy Array which is shaped & looks like this\n",
    "print(x_train.shape)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reshape this dataset. Keras + TensorFlow expects input data to be arranged in a `(qtyOfSamples, width, length, channels)` format -- i.e. a `channels_last` format. `channels` refer to how many 'color' channels are in your model, such as RGB. Because we're working with grayscale images, channels = 1. Thus, the shape of our model should really be (60000, 28, 28, 1) rather than (60000, 28, 28). More information is at https://stackoverflow.com/questions/46145667/shape-of-tensor-for-2d-image-in-keras.\n",
    "\n",
    "If we were working with `Theano`, it'd expect it to be `channels_first` but let's not worry ourselves about this quite yet.\n",
    "\n",
    "And if we were working with a `Recurrent Neural Network`, which is time-series based, you'd want to reshape your data into a `(timeSteps,featuresPerStep)` shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Go back to the Theoratical Notebook\n",
    "I recommend keeping this notebook online so you don't have to re-run the code we ran previously again. In the Theoratical Notebook, go to the very beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Creating the world's simplest Neural Network\n",
    "I'm going to architecture my Neural Network across three functions:\n",
    "\n",
    "1. `fit_model()` will take in the model I create and the training/test input/output datasets. It'll run those datasets through the model to tell me what it predicted.\n",
    "2. `set_params()` which is a function containing a single dictionary of parameters that we'll use to train and build models. This lets us create loops to test different permutations of these functions without duplicating code.\n",
    "3. A variety of functions, each one being a different permutation of a Neural Network.\n",
    "\n",
    "Normally, we'd call these functions from the `main()` function of our code. Given that this is a Jupyter Notebook, we'll just call them straightup from our code blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_model()\n",
    "Because `fit_model()` will never change regardless of what model we'll create, let's create it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, model_params, x_train, y_train, x_test, y_test):\n",
    "    \"\"\" Fits neural network to the given data \n",
    "\n",
    "        Inputs:\n",
    "        model: The Keras Model created\n",
    "        model_params: Dictionary of different parameters used in the model\n",
    "        x_train: Input training data\n",
    "        y_train: Output training data\n",
    "        x_test: Input test data\n",
    "        y_test: Output test data\n",
    "\n",
    "        Outputs:\n",
    "        y_pred_rounded: The predicted output test data (compare against y_test)\n",
    "        metrics: Dictionary stating the 'loss' and 'acc'(uracy) of our model\n",
    "    \"\"\"\n",
    "    # Fits Model\n",
    "    model.fit(x=x_train, y=y_train, epochs=model_params['epoch'], verbose=1)\n",
    "\n",
    "    # Predicts Model\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_rounded = y_pred.round()\n",
    "\n",
    "    # Scores Model on Test Data\n",
    "    metrics = {'acc': 0.0, 'loss': 0.0}\n",
    "    metrics['loss'], metrics['acc'] = model.evaluate(x_test, y_test)\n",
    "    print('\\nAccuracy {} & Loss {}\\n'.format(metrics['acc'], metrics['loss']))\n",
    "\n",
    "    return y_pred_rounded, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.fit()` function as called above expects an `epoch` parameter to be passed in. This is how many iterations we want to run our model over. We'll populate this value later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "The above model requires to inputs, namely the dimensionality of the output space & the activation function. In addition, `fit_model()` expects the number of epochs. Let's set this dictionary now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_params():\n",
    "    model_params = {}\n",
    "\n",
    "    # Parameters for a 1 Layer Model\n",
    "    model_params['dense_1'] = 28\n",
    "    model_params['activate_1'] = 'relu'\n",
    "\n",
    "    # Parameters to Compile Model\n",
    "    model_params['loss'] = 'categorical_crossentropy'\n",
    "    model_params['optimizer'] = 'adam'\n",
    "    \n",
    "    # Parameters to Fit Model\n",
    "    model_params['epoch'] = 3  # Scale as needed. On a 1.6 GHz Intel i5 w. 4 GBs RAM, 1 Epoch = 30-35 Seconds\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "Let's first start by creating a standard Neural Network. Recall that in the prior notebook, we discussed that Neural Networks take an input, modified with weights, and ran through multiple neurons with non-linear activation functions.\n",
    "\n",
    "We'll start by creating a simple sequential neural network with one non-linear layer and then one classification layer at the very end. Generally speaking, I won't hardcode any parameters in this model, choosing to pass them in via a dictionary instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_neural(model_params):\n",
    "    \"\"\" Creates a basic Neural Network \"\"\"\n",
    "    from keras.layers import BatchNormalization, Dense, Flatten, InputLayer\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    # Creates a Sequential Model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Prepares the model to accept an input with a 60000x28x28x1 shape.\n",
    "    model.add(InputLayer(input_shape=(28, 28, 1)))\n",
    "\n",
    "    # Sets Batch Normalization to normalize inputs per epoch\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Runs through a Dense Layer & Activation Function\n",
    "    # The first argument is mandatory: It defines the dimensionality of the output space\n",
    "    # The second (optional) argument sets an Activation Function\n",
    "    model.add(Dense(model_params['dense_1'], activation=model_params['activate_1']))\n",
    "\n",
    "    # Flattens the model to one dimension\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Runs through a Dense Layer for Classification Purposes\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compiles the Model\n",
    "    model.compile(loss=model_params['loss'],\n",
    "                  optimizer=model_params['optimizer'],\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())  # Returns layers created & its dimensions to screen\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running & Fitting the Model\n",
    "And let's call our code now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = set_params()\n",
    "model = basic_neural(model_params)\n",
    "y_pred, metrics = fit_model(model, model_params, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Results\n",
    "We can create a Confusion Plot to show how our model actually performed. This function will create one and it will not change as our models change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Required in Jupyter Notebook to show plots in line\n",
    "%matplotlib inline\n",
    "\n",
    "def conf_matrix(y_test, y_test_predict, classes, title='Confusion Matrix', out=None):\n",
    "    \"\"\" Creates a Confusion Matrix comparing the output test data vs. predicted\n",
    "        output test data. \n",
    "\n",
    "        Inputs:\n",
    "        y_test: Actual output test data\n",
    "        y_test_predict: Predicted output test data\n",
    "        classes: List of valid classes\n",
    "        title: Title of the plot [Default = 'Confusion Matrix']\n",
    "        out: Path to save the plot to [Default = None => Only show it]\n",
    "    \"\"\"\n",
    "    # Converts both output arrays into just one column based on the class\n",
    "    y_test_predict_class = y_test_predict.argmax(1)\n",
    "    y_test_class = y_test.argmax(1)\n",
    "\n",
    "    # Creates confusion matrix\n",
    "    cm_data = confusion_matrix(y_test_class, y_test_predict_class)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plots Confusion Matrix\n",
    "    plt.figure()\n",
    "    plt.imshow(cm_data, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.ylabel('True Label')\n",
    "\n",
    "    # Plots data on chart\n",
    "    thresh = cm_data.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_data.shape[0]), range(cm_data.shape[1])):\n",
    "        plt.text(j, i, format(cm_data[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_data[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Saves or Shows Plot\n",
    "    if out:\n",
    "        plt.savefig(out)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(y_test, y_pred, classes_list, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Go back to the Theoratical Notebook\n",
    "I recommend keeping this notebook online so you don't have to re-run the code we ran previously again. In the Theoratical Notebook, go to the 'Deep Learning Optimizations' section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "# Demonstrating some concepts in our basic neural network\n",
    "\n",
    "** Future Matt: Start running the code now **\n",
    "\n",
    "With a model this simple, it's unlikely (in fact, I know) that any of the concepts we just talked about will actually improve accuracy. But for demonstration sakes, let's show a few of them.\n",
    "\n",
    "Below, we'll add and/or modify the\n",
    "1. Dropout\n",
    "2. Learning Rate of our Optimizer\n",
    "3. Change a Hyperparameter\n",
    "\n",
    "Because this is more computationally heavy and because I'm trying to force everyone to watch neural paint dry, we'll reduce the number of epochs we're running to one. This will impact our accuracy in a negative way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_neural(model_params):\n",
    "    \"\"\" Creates a basic Neural Network \"\"\"\n",
    "    from keras.layers import BatchNormalization, Dense, Dropout, Flatten, InputLayer\n",
    "    from keras.models import Sequential\n",
    "    from keras import optimizers\n",
    "\n",
    "    # Creates a Sequential Model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Prepares the model to accept an input with a 60000x1x28x28 shape.\n",
    "    model.add(InputLayer(input_shape=(28, 28, 1)))\n",
    "\n",
    "    # Sets Batch Normalization to normalize inputs per epoch\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Runs through a Dense Layer & Activation Function\n",
    "    # The first argument is mandatory: It defines the dimensionality of the output space\n",
    "    # The second (optional) argument sets an Activation Function\n",
    "    model.add(Dense(model_params['dense_1'], activation=model_params['activate_1']))\n",
    "\n",
    "    # Dropout Layer\n",
    "    model.add(Dropout(0.10))\n",
    "    \n",
    "    # Flattens the model to one dimension\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Runs through a Dense Layer for Classification Purposes\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Creates an Optimizer (instead of using the default)\n",
    "    # lr is the learning rate. By default, it's set to lr=0.001\n",
    "    adam = optimizers.Adam(lr=0.005)\n",
    "\n",
    "    # Compiles the Model\n",
    "    model.compile(loss=model_params['loss'],\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())  # Returns layers created & its dimensions to screen\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This for loop below will let me test three 'Units'/'Dimensionality' values for the single dense layer we have. Recall that because we're testing one Epoch, our accuracy will take a performance hit. You're welcome to modify it if you have confidence your computer will beat mine (it might!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_params = set_params()\n",
    "\n",
    "# Setting Epochs to 1 to save time\n",
    "model_params['epoch'] = 1\n",
    "\n",
    "# Setting Hyperparameter for the Dense Units\n",
    "for unit in [8, 28, 128]:\n",
    "    # Running & Fitting Model\n",
    "    model_params['dense_1'] = unit\n",
    "    model = two_neural(model_params)\n",
    "    y_pred, metrics = fit_model(model, model_params, x_train, y_train, x_test, y_test)\n",
    "\n",
    "    # Creating Confusiong Matrix\n",
    "    name = '{} Units: {:.2f} Acc & {:.2f} Loss'.format(unit, metrics['acc'], metrics['loss'])\n",
    "    conf_matrix(y_test, y_pred, classes_list, title=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Go back to the Theoratical Notebook\n",
    "I recommend keeping this notebook online so you don't have to re-run the code we ran previously again. In the Theoratical Notebook, go to the 'Convolutional Neural Networks' section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Convolutional Neural Networks\n",
    "\n",
    "** Future Matt: Run the code first **\n",
    "\n",
    "Let's create a convolutional neural network with one convolutional layer. We'll retain our work from the world's simplest neural network, just tacking on a convolutional layer.\n",
    "\n",
    "Recall that the world's simplest neural network had an 85% accuracy and 38% loss. Theoratically, adding a convolutional layer will increase our accuracy and decrease our loss, because we're finding key features from our image now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_params():\n",
    "    model_params = {}\n",
    "\n",
    "    # Parameters for First Convolutional Layer\n",
    "    model_params['conv_1'] = 8  # Number of Neurons\n",
    "    model_params['kernel_1'] = 4  # Kernel Size\n",
    "    model_params['stride_1'] = 1  # Stride\n",
    "    \n",
    "    # Parameters for a 1 Layer Model\n",
    "    model_params['dense_1'] = 8\n",
    "    model_params['activate_1'] = 'relu'\n",
    "\n",
    "    # Parameters to Compile Model\n",
    "    model_params['loss'] = 'categorical_crossentropy'\n",
    "    model_params['optimizer'] = 'adam'\n",
    "    \n",
    "    # Parameters to Fit Model\n",
    "    model_params['epoch'] = 3  # Scale as needed. On a 1.6 GHz Intel i5 w. 4 GBs RAM, 1 Epoch = 30-35 Seconds\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cnn(model_params):\n",
    "    \"\"\" Creates a basic Convolutional Neural Network \"\"\"\n",
    "    from keras.layers import BatchNormalization, Dense, Flatten, InputLayer\n",
    "    from keras.layers.convolutional import Conv2D\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    # Creates a Sequential Model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Prepares the model to accept an input with a 60000x1x28x28 shape.\n",
    "    model.add(InputLayer(input_shape=(28, 28, 1)))\n",
    "\n",
    "    # Sets Batch Normalization to normalize inputs per epoch\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Sets Convolutional Layer\n",
    "    model.add(Conv2D(model_params['conv_1'],\n",
    "                     model_params['kernel_1'],\n",
    "                     strides=model_params['stride_1'],\n",
    "                     padding='same'))\n",
    "\n",
    "    # Runs through a Dense Layer & Activation Function\n",
    "    # The first argument is mandatory: It defines the dimensionality of the output space\n",
    "    # The second (optional) argument sets an Activation Function\n",
    "    model.add(Dense(model_params['dense_1'], activation=model_params['activate_1']))\n",
    "\n",
    "    # Flattens the model to one dimension\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Runs through a Dense Layer for Classification Purposes\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compiles the Model\n",
    "    model.compile(loss=model_params['loss'],\n",
    "                  optimizer=model_params['optimizer'],\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())  # Returns layers created & its dimensions to screen\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = set_params()\n",
    "\n",
    "model = basic_cnn(model_params)\n",
    "y_pred, metrics = fit_model(model, model_params, x_train, y_train, x_test, y_test)\n",
    "conf_matrix(y_test, y_pred, classes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had an accuracy of 88% (+3%) with loss of 32% (-6%). Not only is our model getting better, but that also implies that we're overtraining our model less now, which is always a good thing.\n",
    "\n",
    "Let's add another convolutional layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool_cnn(model_params):\n",
    "    \"\"\" Creates a basic Convolutional Neural Network \"\"\"\n",
    "    from keras.layers import BatchNormalization, Dense, Flatten, InputLayer, MaxPooling2D\n",
    "    from keras.layers.convolutional import Conv2D\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    # Creates a Sequential Model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Prepares the model to accept an input with a 60000x1x28x28 shape.\n",
    "    model.add(InputLayer(input_shape=(28, 28, 1)))\n",
    "\n",
    "    # Sets Batch Normalization to normalize inputs per epoch\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Sets Convolutional Layer\n",
    "    model.add(Conv2D(model_params['conv_1'],\n",
    "                     model_params['kernel_1'],\n",
    "                     strides=model_params['stride_1'],\n",
    "                     padding='same'))\n",
    "    \n",
    "    # Sets MaxPooling Layer\n",
    "    # If pool_size or stride is not set, we default to Stride 2 with a 2x2 Kernel\n",
    "    model.add(MaxPooling2D(padding='same'))\n",
    "\n",
    "    # Sets Convolutional Layer\n",
    "    model.add(Conv2D(model_params['conv_1'],\n",
    "                     model_params['kernel_1'],\n",
    "                     strides=model_params['stride_1'],\n",
    "                     padding='same'))\n",
    "    \n",
    "    # Runs through a Dense Layer & Activation Function\n",
    "    model.add(Dense(model_params['dense_1'], activation=model_params['activate_1']))\n",
    "\n",
    "    # Runs through a Dense Layer & Activation Function\n",
    "    model.add(Dense(model_params['dense_1'], activation=model_params['activate_1']))\n",
    "\n",
    "    # Flattens the model to one dimension\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Runs through a Dense Layer for Classification Purposes\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compiles the Model\n",
    "    model.compile(loss=model_params['loss'],\n",
    "                  optimizer=model_params['optimizer'],\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())  # Returns layers created & its dimensions to screen\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = set_params()\n",
    "\n",
    "model = maxpool_cnn(model_params)\n",
    "y_pred, metrics = fit_model(model, model_params, x_train, y_train, x_test, y_test)\n",
    "conf_matrix(y_test, y_pred, classes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
