{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical View to Neural Networks\n",
    "\n",
    "**Note, I'll be starting in the Keras Notebook first**\n",
    "\n",
    "I'm more application based when it comes to Data Science -- I'm pretty good with applying the mechanics of it, but I must be honest, I'm not the best with memorizing and remembering the theory.\n",
    "\n",
    "And yet, we'd miss a lot of the magic on why Neural Networks work if we ignore the theory. So let's dive into a super basic premise of what a Neural Network actually is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Why Neural Networks?\n",
    "These first two-to-three sections were lovingly stolen from Seth Weidman, who presented a tutorial at PyData NYC 2017. I encourage you to follow him at www.sethweidman.com. I also encourage you to go to PyData at www.pydata.org\n",
    "\n",
    "So why a Neural Network? Why not Machine Learning?\n",
    "\n",
    "### Visual Example\n",
    "Let's say you have the world's most simplistic dataset which looks like this, and you're trying to classify the colored points.\n",
    "![DataPlot](img/xor.png)\n",
    "\n",
    "There isn't a linear classification or logistic classification method to really classify these data points, without being totally absurd or incorrect -- we cannot draw a line, or even a region, to seperate these two classes. \n",
    "\n",
    "\n",
    "### Math Example\n",
    "Alternatively, let's say you have three points of data, each one containing up to three boolean (i.e. True/False) inputs like this:\n",
    "$$ N(1, 0, 0) = 1 $$\n",
    "$$ N(0, 1, 0) = 1 $$\n",
    "$$ N(1, 1, 0) = 1 $$\n",
    "\n",
    "The third function messes up our ability to use Logistic Regression. In other words, there is no parameter b, w1, w2, or w3 such that:\n",
    "$$N(x_1, x_2, x_3) = \\frac{1}{1 + e^{b + w_1 * x_1 + w_2 * x_2 + w_3 * x_3}}$$\n",
    "\n",
    "\n",
    "### Feature Engineering\n",
    "We could use **Feature Engineering** to solve this. To explain Feature Engineering in a few words, it's manually designing what the input should be. You might try to add some discrimination algorithms or emphasize some key features, to modify your inputs so that its key features become more obvious.\n",
    "\n",
    "Deep Learning -- which includes Neural Networks -- is honestly something like **Architecture Engineering** (don't Google that term, I made it up). In Neural Networks, we're effectively playing around either with the parameters or the architecture of our Neural Network so that it better fits our data. In effect, we end up training the computer to do Feature Engineering on its own.\n",
    "\n",
    "Which solution is better? The new hotness is Deep Learning and letting the computer do the feature engineering rather than us. In practice though, there is a time and place for feature engineering. If you suspect your dataset could be defined with a linear or logistic regression by finetuning some of the inputs, it'll probably be faster from a computation perspective to stick to feature engineering and then slide into machine learning. There's not a good 'one rule' here -- whether you go Feature Engieering + ML or go straight to DL is up to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# What is a Neural Network: Forward Propogation\n",
    "Introducing one of the most classic diagrams in visualizing what a Neural Network looks like:\n",
    "![Neural_Intro](img/neural_net_basic.png)\n",
    "**Disclaimer: I had a typo on this diagram. The second set of weights on the right should be 'w', not 'v'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Applying Weights\n",
    "The first step most Neural Networks take is to take the inputs, multiply it by some weight, to obtain a feature.\n",
    "\n",
    "$$ a_1 = x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} $$\n",
    "$$ a_2 = x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} $$\n",
    "$$ a_3 = x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} $$\n",
    "$$ a_4 = x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} $$\n",
    "\n",
    "In the first pass, these weights are probably something absurdly silly (like all 1s, all 0s, etc.). Overtime, they get refined into more helpful units, and we'll talk more about this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "The second step is passing the feature through an **Activation Function**. This is important because if we stuck with the above functions through the entire process, we'd stay Linear and create a Linear Transformation. Chances are, you're here because you want a non-linear transformation and this is where Activation Function comes in.\n",
    "\n",
    "Here's a list of all the possible Activation Functions. Later, we'll talk about the **Relu Function** which is arguably the most popular activation function, although the reason it works so well will probably surprise you.\n",
    "\n",
    "<img src='img/Activation.tiff'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "The Loss is the difference between the estimated result and actual result. Mathematically, this is often the **Mean Squared Error Loss**. The lower the loss is, the better.\n",
    "\n",
    "$$ L = \\frac{1}{2}(y - P)^2 $$\n",
    "\n",
    "There are other Loss Functions you can use and we'll talk more about them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# What is a Neural Network: Forwards to Backwards\n",
    "We continue these algorithms noted above, repeating them until we reach the end of the diagram I showed you before.\n",
    "\n",
    "But you might ask how the weights change -- because by default, they start at silly & unhelpful states. These weights do refine in a process we call `Backpropogation` and/or `Backwards Propogation` (depending who you talk to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does Backpropogation work?\n",
    "Recall that a Neural Network is effectively, some function that takes the inputs of the last function and passes its output to the next function.\n",
    "\n",
    "\\begin{align}\n",
    "A &= a(x, V) \\\\\n",
    "B &= b(A) \\\\\n",
    "C &= c(B, W) \\\\\n",
    "P &= p(C) \\\\\n",
    "L &= l(P)\n",
    "\\end{align}\n",
    "\n",
    "Because these equations are so linked, we can combine them in one line such as below:\n",
    "\n",
    "$$ L = l(p(c(b(a(x, V)), W))) $$\n",
    "\n",
    "Notice that the Weight (W) are related to the functions which create the Neural Network. This implies that **the calculation of those weights in the next iteration (i.e. epoch) can be derived by calculating the partial derivative weight from the partial derivative Loss**. Or in other words,\n",
    "\n",
    "$$ W = W - \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "This works because...\n",
    "* If $\\frac{\\partial L}{\\partial W}$ is a positive number, then we want to _decrease_ the weight, to _decrease_ our loss.\n",
    "* If $\\frac{\\partial L}{\\partial W}$ is a negative number, then we want to _increase_ the weight, to _decrease_ our loss.\n",
    "\n",
    "And we can calculate those partial derivatives via the chain rule. So for example, if we want to obtain the weight 'W' (which is the second set of weights), we'd do:\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial W}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# What is a Neural Network: Backwards Propogation\n",
    "The image below is a reminder of where we're going... just reverse the direction of the arrows.\n",
    "![Neural_Intro](img/neural_net_basic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives Galore\n",
    "Most of this section is about derivatives... so there's high chance I'll fly through this. So if I do, I strongly recommend the article at https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ which explains this process far better than I could."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propogating from Loss to Result\n",
    "Recall that the Loss equation is the Mean Squared Error which was:\n",
    "$$ L = l(P) = \\frac{1}{2}(y - P)^2 $$\n",
    "\n",
    "Thus to obtain the partial derivative of the loss over the result (i.e. the rate of change or the gradient of the loss over the result), the equation becomes:\n",
    "$$ \\frac{\\partial l}{\\partial P} = -(y - P)$$\n",
    "\n",
    "This helps us compute the 'accuracy' of our result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propogating over the Activation Function\n",
    "Recall that our Activation Function was the Sigmoid Function, which was:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "That means its derivative is:\n",
    "$$\\sigma'(x) = \\sigma(x) * (1 - \\sigma(x))$$\n",
    "\n",
    "This helps us compute the 'accuracy' of feature 'C' because we now know the rate of how incorrect we were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propogating over the Weights\n",
    "We now need to compute the relationship of the weight & feature which simply means:\n",
    "$$ \\frac{\\partial c}{\\partial W} $$\n",
    "\n",
    "...Or more complicatedly, that means:\n",
    "$$ \\frac{\\partial c}{\\partial W} = \\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} $$\n",
    "                  \n",
    "But because we got Feature C via:\n",
    "$$\n",
    "\\begin{align}\n",
    "C &= \\begin{bmatrix} c_1 \\end{bmatrix} \\\\ \n",
    "&= c(W) \\\\\n",
    "&= w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "That means that each partial derivative of c over w is equivilant to $B^T$:\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}b_1 \\\\\n",
    "                  b_2 \\\\\n",
    "                  b_3 \\\\\n",
    "                  b_4\n",
    "                  \\end{bmatrix} $$\n",
    "\n",
    "This guides us into creating new weights on the next forward propogation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo\n",
    "Now that we know these concepts, take a look at http://experiments.mostafa.io/public/ffbpann/ for a quick interactive module on these propogation processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Other Layer Types\n",
    "What we described is your basic neuron layer, referred to in Keras as a 'Dense' Neural Layer. There are other layer types, and I specifically want to call out two layers. \n",
    "\n",
    "### Batch Normalization\n",
    "Batch Normalization normalizes the data at each neuron within each epoch (i.e. iteration) of the model. It normalizes it by ensuring the mean is close to 0 and the standard deviation is close to 1.\n",
    "\n",
    "While this additional computation adds run time, this normalization process helps data converge much quicker, which should decrease the overall run time, and give you an opportunity to increase the epochs you can use. This helps us get a better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten\n",
    "Not all data sets and models need this, but with our example, we will. The Flatten Layer flattens data into one dimension. This keeps our model consistently work as we feed it both input data (which is four dimensions) and our expected output data (which is two dimensions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Practical Example: Go to the Keras Notebook\n",
    "Start from the beginning as we build the **World's simplest Neural Network** (I mean it too)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Deep Learning Optimizations\n",
    "Some methods to optimize a Neural Network include:\n",
    "* Learning rate tuning\n",
    "  * Learning rate decay\n",
    "  * Varying learning rates by layer\n",
    "  * Learning rate momentum\n",
    "* Loss Calculation\n",
    "* Preventing Overfitting\n",
    "  * Regularization\n",
    "  * Dropout\n",
    "  * [Not available in Keras yet] Dropconnect\n",
    "* Weight initializations\n",
    "* Different activation functions\n",
    "* Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate + Optimizers\n",
    "By default, our neural network uses a learning rate. **The learning rate defines how the model will refine each individual weight** during backpropogation. \n",
    "\n",
    "Recall that earlier, I provided the equation to refine the Weight Vector. If we added the learning rate of $\\alpha$, that equation would now look like:\n",
    "$$ W = W - \\alpha * \\frac{\\partial l}{\\partial W}$$\n",
    "\n",
    "* When the weight is closer to the input, we want the learning rate to be lower, so that we do not change the weight as much and so we use the input data instead.\n",
    "* When the weight is closer to the output, we want the learning rate to be higher, so that we change the weight more to 'amplify' the input data.\n",
    "\n",
    "In **Keras**, the Learning Rate is defined by an Optimizer. There are several optimizers in Keras and they all either offer options or implement algorithms that control its decay, momentum, and rate per layer. In the neural network we constructed, we used the `adam` optimizer to fine tune the learning rate. The list of those optimizers are at: https://keras.io/optimizers/\n",
    "\n",
    "#### Hyper-Quick Summaries of the primary Optimizers\n",
    "Lovingly stolen and summarized from Keras' Documentation and https://datascience.stackexchange.com/questions/10523/guidelines-for-selecting-an-optimizer-for-training-neural-networks\n",
    "1. `AdaGrad` is better for sparse data. It penalizes the learning rate harshly for parameters which are frequently updated but it also gives more learning rate to sparse parameters. `AdaDelta` is similar but it doesn't require an initial learning rate to be set.\n",
    "2. `RMSProp` is better for recurrent neural networks and those are good for data that changes over time.\n",
    "3. `Adam` is overall the best optimizer. It combines the best of Adadelta and RMSprop.\n",
    "4. `Stochastic Gradient Descent` is very basic and is seldom used now because it uses a global learning rate, thus it doesn't work well when the parameters have different scales. It also generally has a hard time escaping the saddle points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Calculation\n",
    "There are several ways to calculate the loss. We used the `categorical_crossentropy` method in our last neural network.\n",
    "\n",
    "In Keras, you can use the Loss Algorithms on https://keras.io/losses/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "Regularization prevents overfitting by ensuring **no weight is a central point of failure** for the entire network. We do this by adding additional terms to larger weights.\n",
    "\n",
    "`L2 Regularization` (otherwise known as Weight Decay) is the most common type. Here, we augment the the error function with the squared magnitude of all weights in the neural network. This in turn ensures that the network uses _all the weights_ rather than honing in on some of the weights.\n",
    "\n",
    "`L1 Regularization` on the other hand is designed to do the opposite -- so _only the most important weights are used_, which helps the model ignore noise better. This typically performs worse than L2, but this does help at times, especially if you want to know which features are most important. \n",
    "\n",
    "In Keras, you can implement these with the functions at https://keras.io/regularizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Dropout prevents overfitting ensuring **no neuron is a central point of failure** for the entire network. We do this by disconnecting a portion of the neurons (i.e. setting their values to zero) on each forward pass.\n",
    "\n",
    "<img src=\"img/dropout.png\">\n",
    "\n",
    "Based on what we talked about previously, we want to make sure our model learns as much as it can the closer we are to our original input. As such, generally speaking, it makes sense to introduce Dropout on larger networks when some of your final layers are further away from the beginning data.\n",
    "\n",
    "Furthermore, the original paper that introduced Dropout (http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf) actually confirmed that using Dropout with some form of Regularization tends to be ideal.\n",
    "\n",
    "In Keras, you can implement Dropout by following https://keras.io/layers/core/#dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DropConnect\n",
    "Similar to Dropout, but whereas Dropout disabled neurons, here we disable certain weights by setting them to zero.\n",
    "<img src=\"img/drop_connect.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Intialization\n",
    "You can define the initial weights your model will use. Keras lets you do this at: https://keras.io/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions: Relu & the Vanishing Gradient Descent Problem\n",
    "In Keras, a list of Activation Functions are on https://keras.io/activations/ and https://keras.io/layers/advanced-activations/. Attached below is a summarized image of some of the most common Activation Functions from our friends at Wikipedia.\n",
    "\n",
    "<img src='img/Activation.tiff'>\n",
    "\n",
    "As we noted earlier, the Relu Activation Function is one of the most popular activation functions. However, recall that the Relu Activation Function is kind of a linear one. How could it work?\n",
    "\n",
    "Well first, let's take a look at a non-linear activation function, such as tan.\n",
    "\n",
    "<img src='img/grad_descent1.png'> \n",
    "\n",
    "Neural Networks tend to be better with many layers. But if we pick an activation function that is bounded by what numbers it can produce (such as tan), we wouldn't help the model identify differences any easier by adding more layers. \n",
    "\n",
    "<img src='img/grad_descent2.png'> \n",
    "\n",
    "See what I mean? In this example, we're effectively computing S(S(S(S(S(S(S(S(S(S(S(x))))))))))). When we do backpropogation to refine our weights, it'll have a hard time just because there's a lot of computation, over linear derivatives, and the data is 'squished'. This is the **Vanishing Gradient Descent Problem**.\n",
    "\n",
    "The Relu works around this issue though because it's range is effectively 0 to the max(x). Also, because its derivative is not linear persay. Both its forward and backwards propogation approach does not 'squish' the data and this means we get better data as a result.\n",
    "\n",
    "<img src='img/Activation.tiff'>\n",
    "\n",
    "If you want to learn more about this particular problem, I really like the Jupyter Notebook at: https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo\n",
    "Now that we know these concepts, take a look at http://playground.tensorflow.org/ for a quick interactive module on Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Practical Example: Go to the Keras Notebook\n",
    "Go to the **Demonstrating some concepts in our basic neural network** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Convolutional Neural Networks\n",
    "**Convolutional Neural Networks are typically used to do image recognition and/or classification tasks**. They work best when they're analyzing data that is structured -- that is, a particular data point has some relation with the other data points surrounding it.\n",
    "\n",
    "They add two new primary layer types which help extract features from its input data. Those layers are `Convolutional Layers` and `Pooling Layers`.\n",
    "\n",
    "_(All GIFs in this section are obtained from https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59)_\n",
    "\n",
    "\n",
    "\n",
    "### Convolutional Neural Layers\n",
    "The CNN is primarily driven by the `Convolutional Layer`, which effectively is another way to simplify the data.\n",
    "\n",
    "![ConvolutionalLayer](https://cdn-images-1.medium.com/max/1600/1*ZCjPUFrB6eHPRi4eyP6aaA.gif)\n",
    "\n",
    "In the image above, we produce a `convolved feature`/`activation map`/`feature map`. Here are some of the key parts of the image above:\n",
    "* The sliding yellow window is the _Kernel_/_Filter_. This is the multipicative product of weights (denoted in the small red text) and whatever value was originally in that square. These weights change to accomodate what the CNN is learning.\n",
    "* The _Stride_ of the kernel refers to how many 'pixels' it moves in each move\n",
    "\n",
    "The weights are applied just like a standard neural network. So for the first nine pixels in the upper right, we're computing: `(1*1)+(1*0)+(1*1)+(0*0)+(1*1)+(1*0)+(0*1)+(0*0)+(1*1) = 4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "A **Max Pooling** or **Average Pooling** layer creates a kernel on this convolved feature and completely moves it to seperate regions, selecting either the single highest or the average value across all the values within that kernel.\n",
    "\n",
    "![Pooling](https://cdn-images-1.medium.com/max/800/1*Feiexqhmvh9xMGVVJweXhg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lay of the Land\n",
    "You would typically combine these two layers in the beginning, often with several passes. These layers would then feed data into the `Dense` or `Fully Connected` layers we created in the last basic neural network.\n",
    "\n",
    "<img src=\"img/CNNArchitecture.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo\n",
    "Now that we know these concepts, let's take a look at an online interactive visualization.\n",
    "http://scs.ryerson.ca/~aharley/vis/conv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "# CNN Best Practices \n",
    "CNNs are a very new field. There aren't many 'best practices' and researchers are trying to discover how to develop convolutional neural networks for common datasets (such as MNIST -- not to be confused with our Fashion MNIST dataset), let alone other datasets.\n",
    "\n",
    "If you're remotely interested in CNNs, I strongly urge you to reach some of the research papers from the most famous CNNs to understand why they exist. The models get better as the papers get newer, but each paper really builds off of the prior model.\n",
    "\n",
    "Here's a quick overview into all of these models: https://medium.com/@siddharthdas_32104/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5. All of these models have research papers which I believe are all free & open source.\n",
    "\n",
    "Keras does have these models already pre-made for use and its documentation is at https://keras.io/applications/. I would urge you to be careful with these though, becuase these models were designed for the dataset that they were researched with, which probably isn't your dataset. It's better to understand how these models work and see how it might align to your dataset rather than use it outright. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Newest CNN: ResNets (2015)\n",
    "It's claim to fame is its strange architecture. In a ResNet, data can pass into the next filter directly (which is normal) *and* via shortcuts that skip a few layers along the way (which is not normal). Other notes\n",
    "\n",
    "* Each convolutional block uses `Relu` activation functions and does `Batch Normalization` right afterwards.\n",
    "* Dashed arrows means that the dimension of the new output will increase, so we add extra zeros to pad it (This is `Identity Mapping`).\n",
    "* These shortcuts do not necessarily add extra parameters or abnormal computational complexity. What they do bring to the table though is an ability to learn from the 'history' of this data sample getting analyzed from earlier layers.\n",
    "\n",
    "<img src=\"img/ResNet.png\"/>\n",
    "\n",
    "Research Paper: https://arxiv.org/pdf/1512.03385.pdf.\n",
    "\n",
    "To code this in Keras, you cannot use a Sequential model anymore. You'll have to build your CNN layer-by-layer, so that each layer accepts one or multiple inputs. You'll probably need several functions -- one for the convolutional block and one each for the two types of shortcuts you might encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Newest CNN: DenseNets (2016)\n",
    "The problem with ResNets is that the computation can get intense quickly. Researchers realized that many layers were not needed and some of it was redundant. \n",
    "\n",
    "DenseNets `forward feeds` the concated conclusions of the prior dense/neural layers to the next dense/neural layer. As a result, this helps:\n",
    "* Reuses Features & Reduces Parameters (easier to compute)\n",
    "* Alleviates the Vanishing-Gradient Problem\n",
    "* This forward-feeding of data helps improve accuracy and efficiency, assisted via the shorter connections.\n",
    "* Each layer has direct access to one another from the input and output, which creates an implicit deep supervision.\n",
    "\n",
    "<img src=\"img/DenseNets.png\"/>\n",
    "\n",
    "Research Paper: https://arxiv.org/pdf/1608.06993.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Practical Example: Go to the Keras Notebook\n",
    "Go to the **Convolutional Neural Networks** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Recurrent Neural Networks\n",
    "**Recurring Neural Networks are typically used when the sequence of your data is important in how your model performs**. Examples include parsing language (because each word influences the grammatical construct of the next word) or how something might fly in the air over time.\n",
    "\n",
    "Recall that in the World's Simplest Neural Network & Convolutional Neural Network, it really doesn't matter in what order our images come in. A RNN twists this around by taking the output from each layer of neurons and passing it into the activation functions for the next data sample to come in.\n",
    "\n",
    "<img src='img/RNN-PreLSTM.png'/>\n",
    "\n",
    "Image stolen from http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "### The Hidden Layers\n",
    "The big takeaway is that the neuron computing the Activation Function -- known in RNNs as the `Hidden Layer` -- now requires these inputs:\n",
    "* The current time step's input & weights\n",
    "* The prior time step's input & weights\n",
    "  * Which in turn, was influenced by prior time steps.\n",
    "  \n",
    "Here's another GIF to help explain this (lovingly stolen from https://deeplearning4j.org/lstm.html). Here, `x` is the input data, `w` is the weight, `a` is the activation function inside the hidden layer (i.e. neuron), and `b` is the output for said input data.\n",
    "\n",
    "<img src='img/RNN.gif'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation & the Vanishing Gradient Problem\n",
    "Recall that the Vanishing Gradient Problem, for typical neural networks, exists after we add many Dense Layers. That the first few layers might impact the output greatly, but results will soon begin to stagnate with additional layers, and this makes refining our weights harder during backpropogation.\n",
    "\n",
    "In CNNs, we came across two conclusions:\n",
    "1. Using `ReLU` activation functions is ideal, because during backpropogation, the derivative of ReLU does not result in a linear function.\n",
    "2. There is a point where we can have too many layers, so that our outputs begin to stagnate.\n",
    "\n",
    "RNNs are similiar, but in a slightly different way. Let's say we are parsing a sentence in an RNN. The words used depends on the other words in the sentence, but it does not necessarily depend on the words immediately before or after.\n",
    "\n",
    "As discussed, RNNs passes data from each data sample to the next data sample it predicts. This passing of data creates an ever increasing chain rule in size.\n",
    "\n",
    "<img src='img/RNN_Gradient.png'/>\n",
    "\n",
    "**In Neural Networks + CNNs, our results started to stagnate as we add more layers. In RNNs, our results will start to stagnate as this sentence (or whatever data set) gets longer.** This is because each word adds yet another 'dimension' to our calculations. And our weights adapt less efficiently during backpropogation for activation functions that do not derive well, similar to Neural Networks + CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Two Types of RNNs\n",
    "To address the Vanishing Gradient Problem, two new types of RNNs exist nowadays. Both treat the Activation Function -- the hidden layer -- like a black box and utilizes different computation algorithms.\n",
    "\n",
    "### Long Short-Term Memory (LSTM) \n",
    "Without an LSTM, our Hidden Layer would compute one activation function like this.\n",
    "\n",
    "LSTMs were introduced in 1997 (Research Paper: http://www.bioinf.jku.at/publications/older/2604.pdf). I make no doubt that http://colah.github.io/posts/2015-08-Understanding-LSTMs/ explained this process far better than I could ever dream of communicating. But here's a quick stab into what's going on. \n",
    "\n",
    "<img src='img/RNN-LSTM.png'/>\n",
    "\n",
    "Each LSTM Layer takes in:\n",
    "1. The input data from the current time step (x)\n",
    "2. The input data from the prior time step (h)\n",
    "3. The `Cell State` data from the prior time step (C). This helps the model determine if it should rely on the prior time step's data or if it needs to think differently.\n",
    "\n",
    "Going from left to right:\n",
    "* The first branch determines **if we should use the data from the last time step**.\n",
    "  * We compute the sigmoid function to produce a 1 (Yay) or 0 (Boo) over the current timestep's & former timestep's data. This is multiplied against the former cell state's output.\n",
    "  * For example, if we're parsing a sentence and we introduced a new subject midway, it would behoove our model to ignore the gender/traits of the prior subject.\n",
    "* The second & third branch conducts the **update conclusion from the first branch**.\n",
    "  * We compute the sigmoid function (returning 1 or 0) and tanh function (returning from a range of -1 to 1) between our current timestep & former timestep.\n",
    "  * We multiply those values together and add it to the cell state.\n",
    "  * Using our prior example, we would actually compute the effects of ignoring our former subject and focusing on this new subject here.\n",
    "* The final branch determines **which part of the input should be outputted**\n",
    "  * We compute the sigmoid function on our current and former timestep.\n",
    "    * This becomes our output\n",
    "    * This also becomes multiplied by a tanh activation function and gets appended to our cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit (GRU)\n",
    "This is similar to an LSTM, but it's a bit more optimized by combining similar functions together and introducing `Peepholes` where our prior cell state becomes used as opposed to using the prior timestep's data.\n",
    "\n",
    "<img src='img/RNN-GRU.png'/>\n",
    "\n",
    "Research Paper: https://arxiv.org/pdf/1406.1078v3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
